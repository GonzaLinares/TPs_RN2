{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\textit{Embeddings}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "\n",
    "tnews = fetch_20newsgroups(subset=\"train\", remove=('headers', 'footers', 'quotes'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(tnews.keys())\n",
    "pprint(tnews.target_names)\n",
    "pprint(tnews.target)\n",
    "pprint(tnews.target.shape)\n",
    "pprint(tnews.filenames.shape)\n",
    "pprint(tnews.data[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textit{TFIDF/CV - Matriz DT}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf1 = TfidfVectorizer(ngram_range=(1,1), stop_words=\"english\")\n",
    "tfidf2 = TfidfVectorizer(ngram_range=(1,2), stop_words=\"english\")\n",
    "cv1 = CountVectorizer(ngram_range=(1,1), stop_words=\"english\")\n",
    "cv2 = CountVectorizer(ngram_range=(1,2), stop_words=\"english\")\n",
    "\n",
    "vectorization1 = tfidf1.fit_transform(tnews.data)\n",
    "vocabulary1 = tfidf1.get_feature_names_out()\n",
    "vectorization2 = tfidf2.fit_transform(tnews.data)\n",
    "vocabulary2 = tfidf2.get_feature_names_out()\n",
    "\n",
    "vectorization3 = cv1.fit_transform(tnews.data)\n",
    "vocabulary3 = cv1.get_feature_names_out()\n",
    "vectorization4 = cv2.fit_transform(tnews.data)\n",
    "vocabulary4 = cv2.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las vectorizaciones devuelven para cada fila (un documento) la repeticion de cada termino del vocabulario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(vectorization1)\n",
    "pprint(vocabulary1[17000:17010])\n",
    "print(vectorization3[:2]) #Agarro los primeros 2 documentos y sus repeticiones de terminos\n",
    "print(vocabulary1[25717])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similitud coseno entre los primeros 10 documentos del corpus representados como vectores CV o TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matTest = cosine_similarity(vectorization3[:10])\n",
    "\n",
    "names = [tnews.target_names[i] for i in tnews.target[:10]]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8,4)) \n",
    "sns.heatmap(matTest, annot=True, \n",
    "            xticklabels=names,\n",
    "            yticklabels=names,\n",
    "            ax=ax,\n",
    "            cmap=\"icefire\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo mismo que antes pero vemos las diferencias entre CV y TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matTest = cosine_similarity(vectorization1[:10])\n",
    "\n",
    "names = [tnews.target_names[i] for i in tnews.target[:10]]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8,4)) \n",
    "sns.heatmap(matTest, annot=True, \n",
    "            xticklabels=names,\n",
    "            yticklabels=names,\n",
    "            ax=ax,\n",
    "            cmap=\"icefire\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSimilarity(vectors, ref, quant=None):\n",
    "    \n",
    "    matriz_similaridad = cosine_similarity(vectors)\n",
    "\n",
    "    # Create a generator to yield category articles on-the-fly\n",
    "    def category_articles_generator():\n",
    "        for i, category in enumerate(tnews.target_names):\n",
    "            category_indices = [j for j in range(len(tnews.data)) if tnews.target[j] == i]\n",
    "            yield category, category_indices\n",
    "\n",
    "    articles_ref = []\n",
    "    articles_diff = set(range(1, len(tnews.target)+1))\n",
    "\n",
    "    for category, article_indices in category_articles_generator():\n",
    "        if category == ref:\n",
    "            articles_ref.extend(article_indices)\n",
    "            articles_diff.difference_update(article_indices)\n",
    "\n",
    "    reference_article = random.choice(articles_ref)\n",
    "\n",
    "    # Calculate similarities using generators\n",
    "    def similarity_generator(artNum):\n",
    "        vals = []\n",
    "        for artIx in artNum:\n",
    "            if artIx != reference_article:\n",
    "                vals.append(matriz_similaridad[reference_article, artIx-1])\n",
    "        return vals\n",
    "\n",
    "    same = similarity_generator(articles_ref)\n",
    "    diff = similarity_generator(articles_diff)\n",
    "\n",
    "    return same, diff\n",
    "\n",
    "same1, diff1 = getSimilarity(vectorization1, 'sci.space')\n",
    "same2, diff2 = getSimilarity(vectorization2, 'sci.space')\n",
    "same3, diff3 = getSimilarity(vectorization3, 'sci.space')\n",
    "same4, diff4 = getSimilarity(vectorization4, 'sci.space')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def histComp(docs1, docs2, text1, text2):\n",
    "    \n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=0.3, hspace=0.3)\n",
    "\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.hist(list(docs1), edgecolor='black', bins=20, density=True, log=False)\n",
    "    plt.xlabel('SimCos')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title(text1)\n",
    "\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.hist(list(docs2), edgecolor='black', bins=20, density=True, log=False)\n",
    "    plt.xlabel('SimCos')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title(text2)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como es de esperar, los articulos de otras categorias tienen menos similitud coseno que los articulos del mismo tipo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histComp(same1, diff1, 'Misma Cat. tfidf-ngram=1', 'Dist. Cat. tfidf-ngram=1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como la vectorizacion TFIDF penaliza las palabras que aparecen muchas veces en el corpus, se interpreta que los resultados son los esperados, ya que la similitud coseno entre los documentos de la misma categoria usando este metodo son menor en comparacion a la vectorizacion CV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histComp(same1, same3, 'Misma Cat. tfidf-ngram=1', 'Misma Cat. cv-ngram=1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se observo que al agrandar los n_gramas de 1 a 1 y 2, la similitud entre textos de del mismo tipo es mayor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histComp(same3, same4, 'Misma Cat. cv-ngram=1', 'Misma Cat. cv-ngram=2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contrariamente, se observo que al agrandar los n_gramas de 1 a 1 y 2, la similitud entre textos de distinto tipo es menor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histComp(diff3, diff4, 'Dif. Cat. cv-ngram=1', 'Dif. Cat. cv-ngram=2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textit{Ahora con TSVD}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generamos con TSVD una representacion en baja dimensionalidad de los vectores que representan a los documentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd = TruncatedSVD(n_components=32)\n",
    "svd.fit(vectorization1)\n",
    "transformed = svd.transform(vectorization1)\n",
    "transformed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se observa en el heatmap que en esta nueva representacion existen documentos que claramente se asemejan mas entre ellos (como un documento de comp.sys.mac.hardware con otro de la misma categoria) y documentos que parecerian a tender a ser opuestos (como sci.space y comp.graphics)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = cosine_similarity(transformed[:10])\n",
    "\n",
    "names = [tnews.target_names[i] for i in tnews.target[:10]]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8,4)) \n",
    "sns.heatmap(mat, annot=True, \n",
    "            xticklabels=names,\n",
    "            yticklabels=names,\n",
    "            ax=ax,\n",
    "            cmap=\"icefire\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "same1svd, diff1svd = getSimilarity(transformed, 'sci.space')\n",
    "same2svd, diff2svd = getSimilarity(transformed, 'sci.space')\n",
    "same3svd, diff3svd = getSimilarity(transformed, 'sci.space')\n",
    "same4svd, diff4svd = getSimilarity(transformed, 'sci.space')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histComp(same1, diff1, 'Misma Cat. tfidf-ngram=1', 'Dist. Cat. tfidf-ngram=1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histComp(same1, same3, 'Misma Cat. tfidf-ngram=1', 'Misma Cat. cv-ngram=1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histComp(same3, same4, 'Misma Cat. cv-ngram=1', 'Misma Cat. cv-ngram=2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histComp(diff3, diff4, 'Dif. Cat. cv-ngram=1', 'Dif. Cat. cv-ngram=2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textit{Matriz TD}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "termDocMat = vectorization1.transpose()\n",
    "termDocMat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se observa como es de esperar que palabras similares tiene una similitud coseno mayor que aquellas menos relacionadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fromI = 31000\n",
    "\n",
    "matTermDocSim = cosine_similarity(termDocMat[fromI:fromI+10])\n",
    "\n",
    "names = vocabulary1[fromI:fromI+10]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8,4)) \n",
    "sns.heatmap(matTermDocSim, annot=True, \n",
    "            xticklabels=names,\n",
    "            yticklabels=names,\n",
    "            ax=ax,\n",
    "            cmap=\"icefire\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textit{Matriz TC}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a term-class sparse matrix\n",
    "from scipy.sparse import lil_matrix, coo_matrix\n",
    "\n",
    "termClassMat = lil_matrix((len(vocabulary3), len(tnews.target_names)), dtype=np.float32)\n",
    "cx = coo_matrix(vectorization3)\n",
    "\n",
    "# Populate the term-class matrix\n",
    "for i,j,v in zip(cx.row, cx.col, cx.data):\n",
    "    class_i = tnews.target[i]\n",
    "    termClassMat[j, class_i] += v\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtenemos una representacion de cada termino en funcion de los documentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(termClassMat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fromI = 31000\n",
    "\n",
    "matTermClassSim = cosine_similarity(termClassMat[fromI:fromI+10])\n",
    "names = vocabulary3[fromI:fromI+10]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8,4)) \n",
    "sns.heatmap(matTermClassSim, annot=True, \n",
    "            xticklabels=names,\n",
    "            yticklabels=names,\n",
    "            ax=ax,\n",
    "            cmap=\"icefire\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textit{Matriz TT}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import make_sampling_table, skipgrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = Tokenizer(\n",
    "    num_words=10000,\n",
    "    filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "    lower=True,\n",
    "    split=' ',\n",
    "    char_level=False,\n",
    "    oov_token=None,\n",
    "    analyzer=None\n",
    ")\n",
    "\n",
    "token.fit_on_texts(tnews.data) #Tokeniza y cuenta las palabras que mas aparecen, definiendo un vocabulario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sequences = token.texts_to_sequences(tnews.data) #Pasamos los textos a una secuencia de numeros que indican la posicion en el vocabulario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "artIx = 21\n",
    "print(len(train_sequences[artIx]))\n",
    "print(train_sequences[artIx])\n",
    "print(tnews.data[artIx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10001\n",
    "sampling_table = make_sampling_table(vocab_size, sampling_factor=1e-05) #Me devuelve la probabilidad de que tan relevante es la palabra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 100\n",
    "print(sampling_table[t])\n",
    "print(list(token.word_index)[t:t+10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, labels = skipgrams(\n",
    "    sequence=train_sequences[5],\n",
    "    vocabulary_size=vocab_size,\n",
    "    window_size=1,\n",
    "    negative_samples=0,\n",
    "    categorical=False,\n",
    "    sampling_table=sampling_table,\n",
    "    seed=None\n",
    ") #Obtenemos los pares agarrando el central con todos los demas terminos que entren dentro de window_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data[:4])\n",
    "for i in range(len(data[:4])):\n",
    "    print(list(token.word_index)[data[i][0]], list(token.word_index)[data[i][1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textit{PPMI}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_matrix = lil_matrix((vocab_size-1, vocab_size-1))\n",
    "train_sequences = np.hstack(train_sequences)\n",
    "train_sequences = train_sequences.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs, labels = skipgrams(sequence=list(train_sequences), vocabulary_size = vocab_size, window_size=5, negative_samples=0, sampling_table = make_sampling_table(vocab_size, sampling_factor=1), shuffle=False)\n",
    "pairs_u, counts = np.unique(pairs, return_counts=True, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for num, (pair, count) in enumerate(zip(pairs_u, counts)):\n",
    "    if num%1000 ==0:\n",
    "        print(f'\\r{num}', end=\"\")\n",
    "    counts_matrix[pair[0],pair[1]] += count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PMI = counts_matrix[1:,1:].sum(axis=1)/counts_matrix[1:,1:].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = counts_matrix[1:,1:]/counts_matrix[1:,1:].sum()\n",
    "PMI = probs/np.dot(PMI, PMI.T)\n",
    "PMI = np.array(PMI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PMI=PMI*(PMI>1)+1e-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PMI = np.log(PMI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "red = TruncatedSVD(n_components=300)\n",
    "TNG_cv_red = red.fit_transform(PMI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TNG_cv_red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matTNG_cv_red = cosine_similarity(TNG_cv_red[:10])\n",
    "\n",
    "names = [tnews.target_names[i] for i in tnews.target[:10]]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,4)) \n",
    "sns.heatmap(matTNG_cv_red, annot=True, \n",
    "            xticklabels = names,\n",
    "            yticklabels = names,\n",
    "            ax = ax,\n",
    "            cmap = \"icefire\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
