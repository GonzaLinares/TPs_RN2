{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\textit{Predicci√≥n a nivel de palabra}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import wget\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import pad_sequences\n",
    "import codecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip\"\n",
    "filename = wget.download(url=url)\n",
    "with zipfile.ZipFile(filename, 'r') as zip_ref:\n",
    "    zip_ref.extractall(path=\"./\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Sherlock.txt\", 'r', encoding='utf-8') as file:\n",
    "    book = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabSize = 5000\n",
    "token = Tokenizer(num_words=vocabSize,\n",
    "                  filters='!\"#$%&()*+,-/:.;<=>?@[\\\\]^_`{|}~\\t\\n', lower=True,\n",
    "                  split=' ', char_level=False, oov_token=\"UNK\",\n",
    "                  document_count=0)\n",
    "\n",
    "token.fit_on_texts([book])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = token.texts_to_sequences([book])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(text, EOS, SOS, window=100):\n",
    "  inputs = []\n",
    "  outputs = []\n",
    "  \n",
    "  for i in range(len(text)-window):\n",
    "    inputs.append([SOS] + text[i:i+window] + [EOS])\n",
    "    outputs.append(text[i+1:i+window+1] + [EOS])\n",
    "  return list(inputs), list(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 100\n",
    "EOS = vocabSize #End of sequence\n",
    "SOS = vocabSize + 1 #Start of sequence\n",
    "SEP = vocabSize + 2 #Separation of sentences\n",
    "\n",
    "X, y = create_dataset(sequences, EOS, SOS, window_size) #Creamos una secuencia de largo window, con el texto de referencia en X y el texto mas 1 dato mas en y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_validation, y_train, y_validation = train_test_split(X, y, test_size=0.2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths=[len(sequences) for seq in X_train]\n",
    "max_len=120\n",
    "train_sequences=pad_sequences(X_train, maxlen=max_len)\n",
    "test_sequences=pad_sequences(X_validation, maxlen=max_len)\n",
    "train_sequences_y=pad_sequences(y_train, maxlen=max_len)\n",
    "test_sequences_y=pad_sequences(y_validation, maxlen=max_len)\n",
    "reverse_dictionary = token.index_word\n",
    "dictionary = dict([(value, key) for (key, value) in reverse_dictionary.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading word embeddings...\n",
      "found 999995 word vectors\n"
     ]
    }
   ],
   "source": [
    "#load embeddings\n",
    "EMBEDDING_DIR = \"./\"\n",
    "print('loading word embeddings...')\n",
    "embeddings_index = {}\n",
    "f = codecs.open(EMBEDDING_DIR+'wiki-news-300d-1M.vec', encoding='utf-8')\n",
    "for line in f:\n",
    "    values = line.rstrip().rsplit(' ')\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('found %s word vectors' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim=300\n",
    "num_words=len(dictionary)+3\n",
    "embedding_matrix=np.zeros([num_words, embed_dim])\n",
    "for word, idx in dictionary.items():\n",
    "  if idx <= num_words and word in embeddings_index:\n",
    "    embedding_matrix[idx,:]=embeddings_index[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, MaxPooling1D, GlobalMaxPooling1D, Dropout, Dense, Input, Concatenate, Dot, RepeatVector, TimeDistributed, Multiply, Lambda, Flatten, Activation, Reshape, BatchNormalization, LSTM\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.activations import softmax\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "timesteps = 100\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(num_words, embed_dim, weights = [embedding_matrix], input_length=window_size, trainable=True))\n",
    "model.add(LSTM(timesteps, return_sequences=False, name='LSTM1'))\n",
    "model.add(Dense(vocabSize, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='rmsprop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = model.fit(X_train, y_train, batch_size=256, epochs=20, verbose=1, validation_data = (X_validation, y_validation))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
