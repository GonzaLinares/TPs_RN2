{
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c9711fe3b7c94c6386d3a8dcfe1ec0d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "IntProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_01d251b0ffa74820af488d6082e91392",
            "max": 1000000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_aaca3f2433a046bcbccab8758199caae",
            "value": 990000
          }
        },
        "01d251b0ffa74820af488d6082e91392": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aaca3f2433a046bcbccab8758199caae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# $\\textit{By Agustin Gullino, Gonzalo Linares, Mariano Dolhare}$"
      ],
      "metadata": {
        "id": "6MpIznvTDLUD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "from matplotlib import pyplot as plt\n",
        "from collections import Counter\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "Z-MxPGUy_gnh",
        "execution": {
          "iopub.status.busy": "2023-11-02T13:21:29.618866Z",
          "iopub.execute_input": "2023-11-02T13:21:29.619542Z",
          "iopub.status.idle": "2023-11-02T13:21:38.971905Z",
          "shell.execute_reply.started": "2023-11-02T13:21:29.619511Z",
          "shell.execute_reply": "2023-11-02T13:21:38.970834Z"
        },
        "trusted": true,
        "outputId": "57fbf4c4-e872-449f-f909-d1f3d1cc92c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cargamos el embedding de fasttext y el texto del libro de Sherlock Holmes"
      ],
      "metadata": {
        "id": "I80lZcEPJKC4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip\n",
        "!wget -O Sherlock.txt https://www.gutenberg.org/ebooks/48320.txt.utf-8\n",
        "!unzip wiki-news-300d-1M.vec.zip"
      ],
      "metadata": {
        "id": "Nkhi4OZS9A-1",
        "outputId": "dfbb3a42-73c7-4edd-d146-225edfbad534",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.status.busy": "2023-11-02T13:21:38.973676Z",
          "iopub.execute_input": "2023-11-02T13:21:38.974216Z",
          "iopub.status.idle": "2023-11-02T13:22:04.224543Z",
          "shell.execute_reply.started": "2023-11-02T13:21:38.974189Z",
          "shell.execute_reply": "2023-11-02T13:22:04.223576Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "--2023-11-02 13:21:39--  https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip\nResolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 65.8.243.111, 65.8.243.25, 65.8.243.97, ...\nConnecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|65.8.243.111|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 681808098 (650M) [application/zip]\nSaving to: ‘wiki-news-300d-1M.vec.zip’\n\nwiki-news-300d-1M.v 100%[===================>] 650.22M   201MB/s    in 3.6s    \n\n2023-11-02 13:21:43 (181 MB/s) - ‘wiki-news-300d-1M.vec.zip’ saved [681808098/681808098]\n\n--2023-11-02 13:21:44--  https://www.gutenberg.org/ebooks/48320.txt.utf-8\nResolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\nConnecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:443... connected.\nHTTP request sent, awaiting response... 302 Found\nLocation: http://www.gutenberg.org/cache/epub/48320/pg48320.txt [following]\n--2023-11-02 13:21:44--  http://www.gutenberg.org/cache/epub/48320/pg48320.txt\nConnecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:80... connected.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://www.gutenberg.org/cache/epub/48320/pg48320.txt [following]\n--2023-11-02 13:21:44--  https://www.gutenberg.org/cache/epub/48320/pg48320.txt\nConnecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 622314 (608K) [text/plain]\nSaving to: ‘Sherlock.txt’\n\nSherlock.txt        100%[===================>] 607.73K  2.66MB/s    in 0.2s    \n\n2023-11-02 13:21:45 (2.66 MB/s) - ‘Sherlock.txt’ saved [622314/622314]\n\nArchive:  wiki-news-300d-1M.vec.zip\n  inflating: wiki-news-300d-1M.vec   \n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"Sherlock.txt\", 'r', encoding='utf-8') as file:\n",
        "    book = file.read()"
      ],
      "metadata": {
        "id": "WIz1vJra_6tw",
        "execution": {
          "iopub.status.busy": "2023-11-02T13:22:04.226180Z",
          "iopub.execute_input": "2023-11-02T13:22:04.226574Z",
          "iopub.status.idle": "2023-11-02T13:22:04.235570Z",
          "shell.execute_reply.started": "2023-11-02T13:22:04.226539Z",
          "shell.execute_reply": "2023-11-02T13:22:04.234676Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "alphabets= \"([A-Za-z])\"\n",
        "prefixes = \"(Mr|St|Mrs|Ms|Dr|no|No)[.]\"\n",
        "suffixes = \"(Inc|Ltd|Jr|Sr|Co)\"\n",
        "starters = \"(Mr|Mrs|Ms|Dr|Prof|Capt|Cpt|Lt|He\\s|She\\s|It\\s|They\\s|Their\\s|Our\\s|We\\s|But\\s|However\\s|That\\s|This\\s|Wherever)\"\n",
        "acronyms = \"([A-Z][.][A-Z][.](?:[A-Z][.])?)\"\n",
        "websites = \"[.](com|net|org|io|gov|edu|me)\"\n",
        "digits = \"([0-9])\"\n",
        "multiple_dots = r'\\.{2,}'\n",
        "\n",
        "def split_into_sentences(text: str) -> list[str]:\n",
        "    \"\"\"\n",
        "    Split the text into sentences.\n",
        "\n",
        "    If the text contains substrings \"<prd>\" or \"<stop>\", they would lead\n",
        "    to incorrect splitting because they are used as markers for splitting.\n",
        "\n",
        "    :param text: text to be split into sentences\n",
        "    :type text: str\n",
        "\n",
        "    :return: list of sentences\n",
        "    :rtype: list[str]\n",
        "    \"\"\"\n",
        "    text = \" \" + text + \"  \"\n",
        "    text = text.replace(\"\\n\",\" \")\n",
        "    text = re.sub(prefixes,\"\\\\1<prd>\",text)\n",
        "    text = re.sub(websites,\"<prd>\\\\1\",text)\n",
        "    text = re.sub(digits + \"[.]\" + digits,\"\\\\1<prd>\\\\2\",text)\n",
        "    text = re.sub(multiple_dots, lambda match: \"<prd>\" * len(match.group(0)) + \"<stop>\", text)\n",
        "    if \"Ph.D\" in text: text = text.replace(\"Ph.D.\",\"Ph<prd>D<prd>\")\n",
        "    text = re.sub(\"\\s\" + alphabets + \"[.] \",\" \\\\1<prd> \",text)\n",
        "    text = re.sub(acronyms+\" \"+starters,\"\\\\1<stop> \\\\2\",text)\n",
        "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\\\\3<prd>\",text)\n",
        "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\",text)\n",
        "    text = re.sub(\" \"+suffixes+\"[.] \"+starters,\" \\\\1<stop> \\\\2\",text)\n",
        "    text = re.sub(\" \"+suffixes+\"[.]\",\" \\\\1<prd>\",text)\n",
        "    text = re.sub(\" \" + alphabets + \"[.]\",\" \\\\1<prd>\",text)\n",
        "    if \"”\" in text: text = text.replace(\".”\",\"”.\")\n",
        "    if \"\\\"\" in text: text = text.replace(\".\\\"\",\"\\\".\")\n",
        "    if \"!\" in text: text = text.replace(\"!\\\"\",\"\\\"!\")\n",
        "    if \"?\" in text: text = text.replace(\"?\\\"\",\"\\\"?\")\n",
        "    text = text.replace(\".\",\".<stop>\")\n",
        "    text = text.replace(\"?\",\"?<stop>\")\n",
        "    text = text.replace(\"!\",\"!<stop>\")\n",
        "    text = text.replace(\"<prd>\",\".\")\n",
        "    sentences = text.split(\"<stop>\")\n",
        "    sentences = ['<SOS> ' + s.strip() + ' <EOS>' for s in sentences]\n",
        "    if sentences and not sentences[-1]: sentences = sentences[:-1]\n",
        "    return sentences"
      ],
      "metadata": {
        "id": "R2AZITlSf0Ms",
        "execution": {
          "iopub.status.busy": "2023-11-02T13:22:04.237878Z",
          "iopub.execute_input": "2023-11-02T13:22:04.238157Z",
          "iopub.status.idle": "2023-11-02T13:22:04.252937Z",
          "shell.execute_reply.started": "2023-11-02T13:22:04.238134Z",
          "shell.execute_reply": "2023-11-02T13:22:04.251926Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Separamos el libro en oraciones, agregando las palabras \"sos\" y \"eos\". Consideramos varias excepciones para separarlas. El codigo utilizado como base se encuentra en: https://stackoverflow.com/questions/4576077/how-can-i-split-a-text-into-sentences"
      ],
      "metadata": {
        "id": "BshperqPJTAK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "book = split_into_sentences(book)"
      ],
      "metadata": {
        "id": "bIDnuXWggX6O",
        "execution": {
          "iopub.status.busy": "2023-11-02T13:22:04.254153Z",
          "iopub.execute_input": "2023-11-02T13:22:04.254430Z",
          "iopub.status.idle": "2023-11-02T13:22:04.489873Z",
          "shell.execute_reply.started": "2023-11-02T13:22:04.254406Z",
          "shell.execute_reply": "2023-11-02T13:22:04.488685Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "book[1000:1002]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oquhj3AngoVK",
        "outputId": "99c0495e-c5c0-48dc-ecfb-0b17a3b202ab",
        "execution": {
          "iopub.status.busy": "2023-11-02T13:22:04.491482Z",
          "iopub.execute_input": "2023-11-02T13:22:04.491823Z",
          "iopub.status.idle": "2023-11-02T13:22:04.500926Z",
          "shell.execute_reply.started": "2023-11-02T13:22:04.491795Z",
          "shell.execute_reply": "2023-11-02T13:22:04.499785Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 8,
          "output_type": "execute_result",
          "data": {
            "text/plain": "['<SOS> I had come to the conclusion that he had dropped asleep, and indeed was nodding myself, when he suddenly sprang out of his chair with the gesture of a man who has made up his mind, and put his pipe down upon the mantel-piece. <EOS>',\n '<SOS> “Sarasate plays at the St. James’s Hall this afternoon,” he remarked. <EOS>']"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenizamos"
      ],
      "metadata": {
        "id": "I4h0IAfyJ3KZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "maxTokens = 5000\n",
        "token = Tokenizer(num_words=maxTokens,\n",
        "                  filters='!\"“”#$%&()*+,.-/:;=?@[\\\\]^_`{|}~\\t\\n\\ufeff\\u2002', lower=True,\n",
        "                  split=' ', char_level=False, oov_token=None)\n",
        "token.fit_on_texts(book)"
      ],
      "metadata": {
        "id": "Rs9yw_WhPxu7",
        "execution": {
          "iopub.status.busy": "2023-11-02T13:22:04.502046Z",
          "iopub.execute_input": "2023-11-02T13:22:04.502380Z",
          "iopub.status.idle": "2023-11-02T13:22:04.738193Z",
          "shell.execute_reply.started": "2023-11-02T13:22:04.502347Z",
          "shell.execute_reply": "2023-11-02T13:22:04.737170Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = token.texts_to_sequences(book)"
      ],
      "metadata": {
        "id": "HOHSEmy-QAdg",
        "execution": {
          "iopub.status.busy": "2023-11-02T13:22:04.739715Z",
          "iopub.execute_input": "2023-11-02T13:22:04.740177Z",
          "iopub.status.idle": "2023-11-02T13:22:04.945089Z",
          "shell.execute_reply.started": "2023-11-02T13:22:04.740142Z",
          "shell.execute_reply": "2023-11-02T13:22:04.943998Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "maxLen = len(max(sequences, key=len))"
      ],
      "metadata": {
        "id": "crHx5m7YkCNE",
        "execution": {
          "iopub.status.busy": "2023-11-02T13:22:04.946729Z",
          "iopub.execute_input": "2023-11-02T13:22:04.947261Z",
          "iopub.status.idle": "2023-11-02T13:22:04.951917Z",
          "shell.execute_reply.started": "2023-11-02T13:22:04.947234Z",
          "shell.execute_reply": "2023-11-02T13:22:04.950967Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "maxLen"
      ],
      "metadata": {
        "id": "pDGrD4Im-oJl",
        "outputId": "0026473a-d6d9-4ec6-b22e-df144448ea47",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.status.busy": "2023-11-02T13:22:04.959031Z",
          "iopub.execute_input": "2023-11-02T13:22:04.959386Z",
          "iopub.status.idle": "2023-11-02T13:22:04.968033Z",
          "shell.execute_reply.started": "2023-11-02T13:22:04.959320Z",
          "shell.execute_reply": "2023-11-02T13:22:04.966964Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 12,
          "output_type": "execute_result",
          "data": {
            "text/plain": "153"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dataset(sequence):\n",
        "    inputs = []\n",
        "    outputs = []\n",
        "    for i, sent in enumerate(sequence):\n",
        "        for j in range(1, len(sent)-1):\n",
        "          inputs.append(sent[:j])\n",
        "          outputs.append([sent[j]])\n",
        "\n",
        "    return inputs, outputs"
      ],
      "metadata": {
        "id": "XZ73QhT5QAnX",
        "execution": {
          "iopub.status.busy": "2023-11-02T13:22:04.969793Z",
          "iopub.execute_input": "2023-11-02T13:22:04.970196Z",
          "iopub.status.idle": "2023-11-02T13:22:04.977084Z",
          "shell.execute_reply.started": "2023-11-02T13:22:04.970161Z",
          "shell.execute_reply": "2023-11-02T13:22:04.976106Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generamos los datos de train y test para entrenar redes *many to one*"
      ],
      "metadata": {
        "id": "sQ68oQjXJ5Hd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = create_dataset(sequences)\n",
        "x_train = pad_sequences(X, maxlen=maxLen)\n",
        "y_train = np.array(y)\n",
        "x_train, x_test, y_train, y_test = train_test_split(x_train, y_train, test_size=0.3)"
      ],
      "metadata": {
        "id": "TPoB2Q4mQAq2",
        "execution": {
          "iopub.status.busy": "2023-11-02T13:22:04.978940Z",
          "iopub.execute_input": "2023-11-02T13:22:04.979288Z",
          "iopub.status.idle": "2023-11-02T13:22:06.086840Z",
          "shell.execute_reply.started": "2023-11-02T13:22:04.979254Z",
          "shell.execute_reply": "2023-11-02T13:22:06.086015Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "token.sequences_to_texts([X[21]])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_HGVHjN62DXV",
        "outputId": "58ddb7b9-2e88-425e-fca0-b8d9b26a4a03",
        "execution": {
          "iopub.status.busy": "2023-11-02T13:22:06.088093Z",
          "iopub.execute_input": "2023-11-02T13:22:06.088462Z",
          "iopub.status.idle": "2023-11-02T13:22:07.097139Z",
          "shell.execute_reply.started": "2023-11-02T13:22:06.088428Z",
          "shell.execute_reply": "2023-11-02T13:22:07.096116Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 15,
          "output_type": "execute_result",
          "data": {
            "text/plain": "['<sos> the project gutenberg ebook of adventures of sherlock holmes this ebook is for the use of anyone anywhere in the united']"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "token.sequences_to_texts([y[21]])"
      ],
      "metadata": {
        "id": "BO1iDJxGHJ1Q",
        "outputId": "e9c38bf3-f493-4852-8330-1e093ebe1a20",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.status.busy": "2023-11-02T13:22:07.098246Z",
          "iopub.execute_input": "2023-11-02T13:22:07.098547Z",
          "iopub.status.idle": "2023-11-02T13:22:08.840738Z",
          "shell.execute_reply.started": "2023-11-02T13:22:07.098520Z",
          "shell.execute_reply": "2023-11-02T13:22:08.839104Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 16,
          "output_type": "execute_result",
          "data": {
            "text/plain": "['states']"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reverse_dictionary = token.index_word\n",
        "dictionary = dict([(value, key) for (key, value) in reverse_dictionary.items()])\n",
        "num_words=len(dictionary)+1"
      ],
      "metadata": {
        "id": "FKAsqcAja_G9",
        "execution": {
          "iopub.status.busy": "2023-11-02T13:22:08.842171Z",
          "iopub.execute_input": "2023-11-02T13:22:08.842570Z",
          "iopub.status.idle": "2023-11-02T13:22:09.578580Z",
          "shell.execute_reply.started": "2023-11-02T13:22:08.842534Z",
          "shell.execute_reply": "2023-11-02T13:22:09.577505Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import codecs\n",
        "import IPython.display as ipd\n",
        "from IPython.display import Audio, update_display\n",
        "from ipywidgets import IntProgress\n",
        "\n",
        "EMB = \"wiki-news-300d-1M.vec\"\n",
        "N = 1000000\n",
        "\n",
        "#load embeddings\n",
        "print('loading word embeddings...')\n",
        "embeddings_index = {}\n",
        "f = codecs.open(f'./{EMB}', encoding='utf-8')\n",
        "bar = IntProgress(min=0, max=N)\n",
        "ipd.display(bar)\n",
        "\n",
        "i = 0\n",
        "for n, line in enumerate(f):\n",
        "    values = line.rstrip().rsplit(' ')\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "    if n//(N//100) > i:\n",
        "          bar.value = n\n",
        "          i += 1\n",
        "f.close()\n",
        "print('found %s word vectors' % len(embeddings_index))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84,
          "referenced_widgets": [
            "c9711fe3b7c94c6386d3a8dcfe1ec0d3",
            "01d251b0ffa74820af488d6082e91392",
            "aaca3f2433a046bcbccab8758199caae",
            "91109ecf1899455088d9a2a08ce4d47d"
          ]
        },
        "id": "UP3R7ObLqGER",
        "outputId": "ecf5dbdf-d739-4869-b496-478b64e71581",
        "execution": {
          "iopub.status.busy": "2023-11-02T13:22:09.580682Z",
          "iopub.execute_input": "2023-11-02T13:22:09.580997Z",
          "iopub.status.idle": "2023-11-02T13:23:58.956538Z",
          "shell.execute_reply.started": "2023-11-02T13:22:09.580970Z",
          "shell.execute_reply": "2023-11-02T13:23:58.955225Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "loading word embeddings...\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "IntProgress(value=0, max=1000000)",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "91109ecf1899455088d9a2a08ce4d47d"
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "found 999995 word vectors\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generamos la matriz de embeddings"
      ],
      "metadata": {
        "id": "Zz3Kx0oeKFWj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embed_dim=300\n",
        "embedding_matrix=np.zeros([num_words, embed_dim])\n",
        "for word, idx in dictionary.items():\n",
        "  if word in embeddings_index:\n",
        "    embedding_matrix[idx,:]=embeddings_index[word]"
      ],
      "metadata": {
        "id": "cVwzHfDya_Au",
        "execution": {
          "iopub.status.busy": "2023-11-02T13:23:58.958075Z",
          "iopub.execute_input": "2023-11-02T13:23:58.958750Z",
          "iopub.status.idle": "2023-11-02T13:23:58.998421Z",
          "shell.execute_reply.started": "2023-11-02T13:23:58.958713Z",
          "shell.execute_reply": "2023-11-02T13:23:58.997679Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Conv1D, MaxPooling1D, GlobalMaxPooling1D, Dropout, Dense, Input, Concatenate, Dot, RepeatVector, TimeDistributed, Multiply, Lambda, Flatten, Activation, Reshape, BatchNormalization, GRU, SimpleRNN\n",
        "import tensorflow.keras.backend as K\n",
        "from tensorflow.keras.activations import softmax\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
        "\n",
        "nb_words = len(embedding_matrix) #Vocabulary size\n",
        "embed_dim = len(embedding_matrix[0]) #Vectorization dim\n",
        "value_dim = 50\n",
        "maxLen = len(x_train[0])"
      ],
      "metadata": {
        "id": "koANcVKYYk71",
        "execution": {
          "iopub.status.busy": "2023-11-02T13:23:58.999718Z",
          "iopub.execute_input": "2023-11-02T13:23:59.000371Z",
          "iopub.status.idle": "2023-11-02T13:23:59.010950Z",
          "shell.execute_reply.started": "2023-11-02T13:23:59.000332Z",
          "shell.execute_reply": "2023-11-02T13:23:59.009946Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Luego de entrenar varias redes variando sus hiperparametros, concluimos que diejon mejores resultados la LSTM, seguida de la GRU."
      ],
      "metadata": {
        "id": "0Z41mSgYKwcM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Red LSTM"
      ],
      "metadata": {
        "id": "3yy8RtyzKOCR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_layer = Input(shape=(maxLen,)) #Cantidad maxima de la frase de entrada, estos son los timesteps para este caso\n",
        "embedding_layer = Embedding(nb_words, embed_dim, weights=[embedding_matrix], input_length=maxLen, trainable=False, mask_zero=True)(input_layer)\n",
        "lstm = LSTM(value_dim, return_sequences=False, activation=\"tanh\", recurrent_dropout=0.4)(embedding_layer)\n",
        "drop = Dropout(0.4)(lstm)\n",
        "dense3 = Dense(100, activation='relu')(drop)\n",
        "drop3 = Dropout(0.4)(dense3)\n",
        "dense4 = Dense(nb_words, activation='softmax')(drop3)\n",
        "model = Model(inputs=input_layer, outputs=dense4)"
      ],
      "metadata": {
        "id": "MrF-WYsn45kL",
        "execution": {
          "iopub.status.busy": "2023-11-02T13:23:59.012106Z",
          "iopub.execute_input": "2023-11-02T13:23:59.012343Z",
          "iopub.status.idle": "2023-11-02T13:24:03.865678Z",
          "shell.execute_reply.started": "2023-11-02T13:23:59.012321Z",
          "shell.execute_reply": "2023-11-02T13:24:03.864861Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Red GRU"
      ],
      "metadata": {
        "id": "AmkW08HEKTjS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model2 = tf.keras.Sequential()\n",
        "model2.add(Input(shape=(maxLen,)))\n",
        "model2.add(Embedding(nb_words, embed_dim, weights=[embedding_matrix], input_length=maxLen, trainable=True, mask_zero=True))\n",
        "model2.add(GRU(value_dim, return_sequences=False, activation=\"tanh\"))\n",
        "model2.add(Dropout(0.4))\n",
        "model2.add(Dense(100, activation='relu'))\n",
        "model2.add(Dropout(0.4))\n",
        "model2.add(Dense(nb_words, activation='softmax'))"
      ],
      "metadata": {
        "id": "SNqHy9De45kL",
        "execution": {
          "iopub.status.busy": "2023-11-02T14:25:53.242846Z",
          "iopub.execute_input": "2023-11-02T14:25:53.243156Z",
          "iopub.status.idle": "2023-11-02T14:25:54.199439Z",
          "shell.execute_reply.started": "2023-11-02T14:25:53.243128Z",
          "shell.execute_reply": "2023-11-02T14:25:54.198672Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Red Convolucional"
      ],
      "metadata": {
        "id": "AllA5CF0KU6Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_filters=12\n",
        "model3 = tf.keras.Sequential()\n",
        "model3.add(Input(shape=(maxLen,)))\n",
        "model3.add(Embedding(nb_words, embed_dim, weights=[embedding_matrix], input_length=maxLen, trainable=True, mask_zero=True))\n",
        "model3.add(Conv1D(num_filters, 7, activation='relu', padding='same'))\n",
        "model3.add(MaxPooling1D(2))\n",
        "model3.add(Conv1D(num_filters*2, 7, activation='relu', padding='same'))\n",
        "model3.add(GlobalMaxPooling1D())\n",
        "model3.add(Dropout(0.4))\n",
        "model3.add(Dense(100, activation='relu'))\n",
        "model3.add(Dropout(0.4))\n",
        "model3.add(Dense(nb_words, activation='softmax'))"
      ],
      "metadata": {
        "id": "3-Zg9xSO45kL",
        "execution": {
          "iopub.status.busy": "2023-11-02T13:24:04.147593Z",
          "iopub.execute_input": "2023-11-02T13:24:04.147913Z",
          "iopub.status.idle": "2023-11-02T13:24:04.248647Z",
          "shell.execute_reply.started": "2023-11-02T13:24:04.147887Z",
          "shell.execute_reply": "2023-11-02T13:24:04.247649Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Red SimpleRNN"
      ],
      "metadata": {
        "id": "OUJv34HpKXl9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model4 = tf.keras.Sequential()\n",
        "model4.add(Input(shape=(maxLen,)))\n",
        "model4.add(Embedding(nb_words, embed_dim, weights=[embedding_matrix], input_length=maxLen, trainable=True, mask_zero=True))\n",
        "model4.add(SimpleRNN(value_dim, return_sequences=False, activation=\"tanh\"))\n",
        "model4.add(Dropout(0.4))\n",
        "model4.add(Dense(100, activation='relu'))\n",
        "model4.add(Dropout(0.4))\n",
        "model4.add(Dense(nb_words, activation='softmax'))"
      ],
      "metadata": {
        "id": "huOT_2CF45kL",
        "execution": {
          "iopub.status.busy": "2023-11-02T13:24:04.249731Z",
          "iopub.execute_input": "2023-11-02T13:24:04.250016Z",
          "iopub.status.idle": "2023-11-02T13:24:04.367215Z",
          "shell.execute_reply.started": "2023-11-02T13:24:04.249990Z",
          "shell.execute_reply": "2023-11-02T13:24:04.366466Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#val_loss: 5.38\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "85gXMeBuhUj1",
        "outputId": "c6a14264-3127-4a5b-f944-18decd11087b",
        "execution": {
          "iopub.status.busy": "2023-11-02T13:24:04.368276Z",
          "iopub.execute_input": "2023-11-02T13:24:04.368557Z",
          "iopub.status.idle": "2023-11-02T13:24:04.391974Z",
          "shell.execute_reply.started": "2023-11-02T13:24:04.368532Z",
          "shell.execute_reply": "2023-11-02T13:24:04.391081Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Model: \"model\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_1 (InputLayer)        [(None, 153)]             0         \n                                                                 \n embedding (Embedding)       (None, 153, 300)          2652900   \n                                                                 \n lstm (LSTM)                 (None, 50)                70200     \n                                                                 \n dropout (Dropout)           (None, 50)                0         \n                                                                 \n dense (Dense)               (None, 100)               5100      \n                                                                 \n dropout_1 (Dropout)         (None, 100)               0         \n                                                                 \n dense_1 (Dense)             (None, 8843)              893143    \n                                                                 \n=================================================================\nTotal params: 3,621,343\nTrainable params: 968,443\nNon-trainable params: 2,652,900\n_________________________________________________________________\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#val_loss: 5.55\n",
        "model2.summary()"
      ],
      "metadata": {
        "id": "HsgjMB2a45kL",
        "outputId": "5b5e782c-7aee-4be9-f381-d0a945368d46",
        "execution": {
          "iopub.status.busy": "2023-11-02T13:24:04.393118Z",
          "iopub.execute_input": "2023-11-02T13:24:04.393404Z",
          "iopub.status.idle": "2023-11-02T13:24:04.439486Z",
          "shell.execute_reply.started": "2023-11-02T13:24:04.393379Z",
          "shell.execute_reply": "2023-11-02T13:24:04.438605Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Model: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n embedding_1 (Embedding)     (None, 153, 300)          2652900   \n                                                                 \n gru (GRU)                   (None, 50)                52800     \n                                                                 \n dropout_2 (Dropout)         (None, 50)                0         \n                                                                 \n dense_2 (Dense)             (None, 100)               5100      \n                                                                 \n dropout_3 (Dropout)         (None, 100)               0         \n                                                                 \n dense_3 (Dense)             (None, 8843)              893143    \n                                                                 \n=================================================================\nTotal params: 3,603,943\nTrainable params: 3,603,943\nNon-trainable params: 0\n_________________________________________________________________\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#val_loss: 6.3046\n",
        "model3.summary()"
      ],
      "metadata": {
        "id": "kN_sw3a245kM",
        "outputId": "6eaa9f04-4731-4aff-b10e-9021602e5efb",
        "execution": {
          "iopub.status.busy": "2023-11-02T13:24:04.440848Z",
          "iopub.execute_input": "2023-11-02T13:24:04.441224Z",
          "iopub.status.idle": "2023-11-02T13:24:04.490236Z",
          "shell.execute_reply.started": "2023-11-02T13:24:04.441189Z",
          "shell.execute_reply": "2023-11-02T13:24:04.489408Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Model: \"sequential_1\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n embedding_2 (Embedding)     (None, 153, 300)          2652900   \n                                                                 \n conv1d (Conv1D)             (None, 153, 12)           25212     \n                                                                 \n max_pooling1d (MaxPooling1D  (None, 76, 12)           0         \n )                                                               \n                                                                 \n conv1d_1 (Conv1D)           (None, 76, 24)            2040      \n                                                                 \n global_max_pooling1d (Globa  (None, 24)               0         \n lMaxPooling1D)                                                  \n                                                                 \n dropout_4 (Dropout)         (None, 24)                0         \n                                                                 \n dense_4 (Dense)             (None, 100)               2500      \n                                                                 \n dropout_5 (Dropout)         (None, 100)               0         \n                                                                 \n dense_5 (Dense)             (None, 8843)              893143    \n                                                                 \n=================================================================\nTotal params: 3,575,795\nTrainable params: 3,575,795\nNon-trainable params: 0\n_________________________________________________________________\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#val_loss: 5.53\n",
        "model4.summary()"
      ],
      "metadata": {
        "id": "q3MyXKa545kM",
        "outputId": "124a0144-9e60-43b7-d9cb-2c2b93894882",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.status.busy": "2023-11-02T13:24:04.491872Z",
          "iopub.execute_input": "2023-11-02T13:24:04.492148Z",
          "iopub.status.idle": "2023-11-02T13:24:04.514750Z",
          "shell.execute_reply.started": "2023-11-02T13:24:04.492124Z",
          "shell.execute_reply": "2023-11-02T13:24:04.513872Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Model: \"sequential_2\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n embedding_3 (Embedding)     (None, 153, 300)          2652900   \n                                                                 \n simple_rnn (SimpleRNN)      (None, 50)                17550     \n                                                                 \n dropout_6 (Dropout)         (None, 50)                0         \n                                                                 \n dense_6 (Dense)             (None, 100)               5100      \n                                                                 \n dropout_7 (Dropout)         (None, 100)               0         \n                                                                 \n dense_7 (Dense)             (None, 8843)              893143    \n                                                                 \n=================================================================\nTotal params: 3,568,693\nTrainable params: 3,568,693\nNon-trainable params: 0\n_________________________________________________________________\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "callbackROP = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=5)\n",
        "callbackES = EarlyStopping(monitor='val_loss', patience=10)\n",
        "callbackCHK = ModelCheckpoint(filepath='/tmp/checkpoint', save_weights_only=True, monitor='val_loss', mode='min', save_best_only=True)"
      ],
      "metadata": {
        "id": "j0JRb8SNhUj1",
        "execution": {
          "iopub.status.busy": "2023-11-02T13:24:04.515905Z",
          "iopub.execute_input": "2023-11-02T13:24:04.516241Z",
          "iopub.status.idle": "2023-11-02T13:24:04.522293Z",
          "shell.execute_reply.started": "2023-11-02T13:24:04.516210Z",
          "shell.execute_reply": "2023-11-02T13:24:04.521144Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer='adam')"
      ],
      "metadata": {
        "id": "ZosLpsQfwcdb",
        "execution": {
          "iopub.status.busy": "2023-11-02T13:24:04.526918Z",
          "iopub.execute_input": "2023-11-02T13:24:04.527240Z",
          "iopub.status.idle": "2023-11-02T13:24:04.547201Z",
          "shell.execute_reply.started": "2023-11-02T13:24:04.527204Z",
          "shell.execute_reply": "2023-11-02T13:24:04.546242Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(x=x_train, y=y_train, validation_data=(x_test, y_test), batch_size=512, epochs=50, callbacks=[callbackROP, callbackES, callbackCHK])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "id": "QdyPh--mhUj1",
        "outputId": "b61c91c6-bf03-4ae3-ba2b-c10bce84d3e9",
        "execution": {
          "iopub.status.busy": "2023-11-02T13:24:04.548695Z",
          "iopub.execute_input": "2023-11-02T13:24:04.548999Z",
          "iopub.status.idle": "2023-11-02T14:25:53.240707Z",
          "shell.execute_reply.started": "2023-11-02T13:24:04.548973Z",
          "shell.execute_reply": "2023-11-02T14:25:53.239495Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Epoch 1/50\n147/147 [==============================] - 96s 599ms/step - loss: 7.1174 - val_loss: 6.4912 - lr: 0.0010\nEpoch 2/50\n147/147 [==============================] - 88s 602ms/step - loss: 6.3946 - val_loss: 6.3464 - lr: 0.0010\nEpoch 3/50\n147/147 [==============================] - 88s 602ms/step - loss: 6.2662 - val_loss: 6.2560 - lr: 0.0010\nEpoch 4/50\n147/147 [==============================] - 88s 596ms/step - loss: 6.1677 - val_loss: 6.1288 - lr: 0.0010\nEpoch 5/50\n147/147 [==============================] - 87s 591ms/step - loss: 6.0416 - val_loss: 5.9834 - lr: 0.0010\nEpoch 6/50\n147/147 [==============================] - 87s 590ms/step - loss: 5.9274 - val_loss: 5.8830 - lr: 0.0010\nEpoch 7/50\n147/147 [==============================] - 87s 591ms/step - loss: 5.8327 - val_loss: 5.8030 - lr: 0.0010\nEpoch 8/50\n147/147 [==============================] - 87s 590ms/step - loss: 5.7489 - val_loss: 5.7384 - lr: 0.0010\nEpoch 9/50\n147/147 [==============================] - 86s 588ms/step - loss: 5.6828 - val_loss: 5.6918 - lr: 0.0010\nEpoch 10/50\n147/147 [==============================] - 87s 591ms/step - loss: 5.6209 - val_loss: 5.6559 - lr: 0.0010\nEpoch 11/50\n147/147 [==============================] - 86s 584ms/step - loss: 5.5763 - val_loss: 5.6213 - lr: 0.0010\nEpoch 12/50\n147/147 [==============================] - 86s 584ms/step - loss: 5.5254 - val_loss: 5.5939 - lr: 0.0010\nEpoch 13/50\n147/147 [==============================] - 86s 585ms/step - loss: 5.4899 - val_loss: 5.5688 - lr: 0.0010\nEpoch 14/50\n147/147 [==============================] - 84s 572ms/step - loss: 5.4525 - val_loss: 5.5471 - lr: 0.0010\nEpoch 15/50\n147/147 [==============================] - 84s 570ms/step - loss: 5.4152 - val_loss: 5.5252 - lr: 0.0010\nEpoch 16/50\n147/147 [==============================] - 84s 571ms/step - loss: 5.3866 - val_loss: 5.5082 - lr: 0.0010\nEpoch 17/50\n147/147 [==============================] - 84s 569ms/step - loss: 5.3533 - val_loss: 5.4903 - lr: 0.0010\nEpoch 18/50\n147/147 [==============================] - 83s 567ms/step - loss: 5.3263 - val_loss: 5.4749 - lr: 0.0010\nEpoch 19/50\n147/147 [==============================] - 83s 566ms/step - loss: 5.2977 - val_loss: 5.4634 - lr: 0.0010\nEpoch 20/50\n147/147 [==============================] - 83s 566ms/step - loss: 5.2727 - val_loss: 5.4494 - lr: 0.0010\nEpoch 21/50\n147/147 [==============================] - 84s 571ms/step - loss: 5.2469 - val_loss: 5.4388 - lr: 0.0010\nEpoch 22/50\n147/147 [==============================] - 83s 565ms/step - loss: 5.2257 - val_loss: 5.4307 - lr: 0.0010\nEpoch 23/50\n147/147 [==============================] - 83s 562ms/step - loss: 5.2015 - val_loss: 5.4200 - lr: 0.0010\nEpoch 24/50\n147/147 [==============================] - 83s 568ms/step - loss: 5.1830 - val_loss: 5.4146 - lr: 0.0010\nEpoch 25/50\n147/147 [==============================] - 83s 564ms/step - loss: 5.1620 - val_loss: 5.4101 - lr: 0.0010\nEpoch 26/50\n147/147 [==============================] - 83s 565ms/step - loss: 5.1413 - val_loss: 5.4023 - lr: 0.0010\nEpoch 27/50\n147/147 [==============================] - 83s 564ms/step - loss: 5.1278 - val_loss: 5.4005 - lr: 0.0010\nEpoch 28/50\n147/147 [==============================] - 82s 561ms/step - loss: 5.1136 - val_loss: 5.3956 - lr: 0.0010\nEpoch 29/50\n147/147 [==============================] - 83s 563ms/step - loss: 5.0921 - val_loss: 5.3928 - lr: 0.0010\nEpoch 30/50\n147/147 [==============================] - 83s 562ms/step - loss: 5.0771 - val_loss: 5.3912 - lr: 0.0010\nEpoch 31/50\n147/147 [==============================] - 82s 561ms/step - loss: 5.0614 - val_loss: 5.3889 - lr: 0.0010\nEpoch 32/50\n147/147 [==============================] - 83s 565ms/step - loss: 5.0438 - val_loss: 5.3868 - lr: 0.0010\nEpoch 33/50\n147/147 [==============================] - 83s 564ms/step - loss: 5.0296 - val_loss: 5.3868 - lr: 0.0010\nEpoch 34/50\n147/147 [==============================] - 83s 567ms/step - loss: 5.0229 - val_loss: 5.3864 - lr: 0.0010\nEpoch 35/50\n147/147 [==============================] - 83s 562ms/step - loss: 5.0111 - val_loss: 5.3897 - lr: 0.0010\nEpoch 36/50\n147/147 [==============================] - 83s 562ms/step - loss: 4.9961 - val_loss: 5.3889 - lr: 0.0010\nEpoch 37/50\n147/147 [==============================] - 82s 560ms/step - loss: 4.9816 - val_loss: 5.3865 - lr: 0.0010\nEpoch 38/50\n147/147 [==============================] - 82s 561ms/step - loss: 4.9734 - val_loss: 5.3909 - lr: 0.0010\nEpoch 39/50\n147/147 [==============================] - 83s 562ms/step - loss: 4.9642 - val_loss: 5.3883 - lr: 0.0010\nEpoch 40/50\n147/147 [==============================] - 82s 560ms/step - loss: 4.9354 - val_loss: 5.3913 - lr: 5.0000e-04\nEpoch 41/50\n147/147 [==============================] - 82s 559ms/step - loss: 4.9259 - val_loss: 5.3943 - lr: 5.0000e-04\nEpoch 42/50\n147/147 [==============================] - 82s 559ms/step - loss: 4.9178 - val_loss: 5.3983 - lr: 5.0000e-04\nEpoch 43/50\n147/147 [==============================] - 83s 562ms/step - loss: 4.9140 - val_loss: 5.4012 - lr: 5.0000e-04\nEpoch 44/50\n147/147 [==============================] - 82s 557ms/step - loss: 4.9082 - val_loss: 5.4031 - lr: 5.0000e-04\n",
          "output_type": "stream"
        },
        {
          "execution_count": 31,
          "output_type": "execute_result",
          "data": {
            "text/plain": "<keras.callbacks.History at 0x788a8c6c74f0>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Greedy search + ArgMax"
      ],
      "metadata": {
        "id": "3gpFgMjFeLcT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dada la naturaleza de la generacion de oraciones, este metodo demostro generar recurrentemente loops de palabras. Esto es debido a que siempre agarramos la de mayor probabilidad y siendo que la distribucion de palabras en estas redes que no estan muy overfitteadas suelen tener ciertas palabras muy probables, entonces termina generando el efecto de que a pesar de que agregamos nuevas parlabras para predecir, estas no son lo suficientes para que la red genere estados diferentes."
      ],
      "metadata": {
        "id": "RsTA05l7KrzV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def greedy_search(question, outputLen, model):\n",
        "\n",
        "  out = 0\n",
        "  response = []\n",
        "  reply = question\n",
        "  finished = False\n",
        "\n",
        "  while out != token.texts_to_sequences([\"<eos>\"])[0][0] and len(response) < outputLen:\n",
        "      out = pad_sequences(reply, maxlen=maxLen)\n",
        "      out = model.predict(out, verbose=0)\n",
        "      out = int(np.argmax(out))\n",
        "      reply[0] += [out]\n",
        "      response.append(out)\n",
        "\n",
        "  return response"
      ],
      "metadata": {
        "id": "N3RUZ8SUeMMm",
        "execution": {
          "iopub.status.busy": "2023-11-02T14:46:39.408991Z",
          "iopub.execute_input": "2023-11-02T14:46:39.409341Z",
          "iopub.status.idle": "2023-11-02T14:46:39.415760Z",
          "shell.execute_reply.started": "2023-11-02T14:46:39.409317Z",
          "shell.execute_reply": "2023-11-02T14:46:39.414767Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = token.texts_to_sequences([\"<sos> sherlock\"])"
      ],
      "metadata": {
        "id": "3A8POxoleX2i",
        "execution": {
          "iopub.status.busy": "2023-11-02T14:46:51.553686Z",
          "iopub.execute_input": "2023-11-02T14:46:51.554294Z",
          "iopub.status.idle": "2023-11-02T14:46:51.558643Z",
          "shell.execute_reply.started": "2023-11-02T14:46:51.554261Z",
          "shell.execute_reply": "2023-11-02T14:46:51.557587Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def from_seq_to_text(seq):\n",
        "    return token.sequences_to_texts([seq])\n",
        "\n",
        "response = greedy_search(question, 15, model)\n",
        "rta = from_seq_to_text(response)\n",
        "rta"
      ],
      "metadata": {
        "id": "gZIlKOhaecsY",
        "outputId": "58dd2844-4295-4520-bf2c-5196d572cf14",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.status.busy": "2023-11-02T14:46:52.703535Z",
          "iopub.execute_input": "2023-11-02T14:46:52.704308Z",
          "iopub.status.idle": "2023-11-02T14:46:54.188200Z",
          "shell.execute_reply.started": "2023-11-02T14:46:52.704276Z",
          "shell.execute_reply": "2023-11-02T14:46:54.187180Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 149,
          "output_type": "execute_result",
          "data": {
            "text/plain": "['holmes was a little of the matter of the matter of the matter of the']"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Greedy search + T"
      ],
      "metadata": {
        "id": "cp-mOs7oPJr6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para este caso, se obtuvieron mejores resultados que con el ArgMax. En primer lugar se noto que si se ajusta la temperatura a un valor adecuado, los loops de palabras practicamente desaparecian. Sin embargo, si subiamos mucho la temperatura, los modelos comenzaban a generar palabras de forma practicamente aleatorias, provocando incoherencias gramaticales."
      ],
      "metadata": {
        "id": "C6PkxpYdLzAS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def changeTemp(a, temperature=1.0, epsilon=1e-10):\n",
        "    a = np.log(a) / temperature\n",
        "    sampled_temp = np.exp(a+1e-10)/(np.exp(a+1e-10).sum())\n",
        "    sampled_temp = sampled_temp/sampled_temp.sum()\n",
        "    return sampled_temp"
      ],
      "metadata": {
        "id": "HvFeRI-lPJr7",
        "execution": {
          "iopub.status.busy": "2023-11-02T14:47:05.520376Z",
          "iopub.execute_input": "2023-11-02T14:47:05.521318Z",
          "iopub.status.idle": "2023-11-02T14:47:05.527557Z",
          "shell.execute_reply.started": "2023-11-02T14:47:05.521275Z",
          "shell.execute_reply": "2023-11-02T14:47:05.526338Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def greedy_search_withT(question, model, max_len=15, T=1.0):\n",
        "\n",
        "  out = 0\n",
        "  response = []\n",
        "  reply = question\n",
        "  finished = False\n",
        "  eof = token.texts_to_sequences([\"<eos>\"])[0][0]\n",
        "\n",
        "  while out != eof and len(response) < max_len:\n",
        "\n",
        "      out = pad_sequences(reply, maxlen=maxLen)\n",
        "      out = model.predict(out, verbose=0)\n",
        "      temp = changeTemp(out[0], temperature=T)\n",
        "      out = np.random.choice(range(len(temp)), p=temp)\n",
        "      reply[0] += [out]\n",
        "      if out in response:\n",
        "        finished = True\n",
        "      else:\n",
        "        response.append(out)\n",
        "\n",
        "  return response"
      ],
      "metadata": {
        "id": "DbUWjR8m4rE1",
        "execution": {
          "iopub.status.busy": "2023-11-02T14:47:07.165024Z",
          "iopub.execute_input": "2023-11-02T14:47:07.165727Z",
          "iopub.status.idle": "2023-11-02T14:47:07.172614Z",
          "shell.execute_reply.started": "2023-11-02T14:47:07.165697Z",
          "shell.execute_reply": "2023-11-02T14:47:07.171682Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vemos el efecto del cambio de temperatura de la *softmax*."
      ],
      "metadata": {
        "id": "VVZTsnOVMpC8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "arr1 = [1, 1.2, 0.5, 0.6, 0.8]\n",
        "plt.figure()\n",
        "plt.bar(range(len(arr1)), arr1, alpha=0.5)\n",
        "arr2 = changeTemp(arr1, temperature=2.1)\n",
        "plt.bar(range(len(arr2)), arr2, alpha=0.5)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "tLwZCSGoY17b",
        "outputId": "e6ae3c80-7ef9-4607-83f5-a0bb2ae46268",
        "execution": {
          "iopub.status.busy": "2023-11-02T14:47:09.311933Z",
          "iopub.execute_input": "2023-11-02T14:47:09.312635Z",
          "iopub.status.idle": "2023-11-02T14:47:09.564950Z",
          "shell.execute_reply.started": "2023-11-02T14:47:09.312586Z",
          "shell.execute_reply": "2023-11-02T14:47:09.563903Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<Figure size 640x480 with 1 Axes>",
            "image/png": "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"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question = token.texts_to_sequences([\"<sos> sherlock was walking\"])"
      ],
      "metadata": {
        "id": "rqHcaXMI5yu0",
        "execution": {
          "iopub.status.busy": "2023-11-02T14:48:40.819993Z",
          "iopub.execute_input": "2023-11-02T14:48:40.820344Z",
          "iopub.status.idle": "2023-11-02T14:48:40.824794Z",
          "shell.execute_reply.started": "2023-11-02T14:48:40.820313Z",
          "shell.execute_reply": "2023-11-02T14:48:40.823844Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def from_seq_to_text(seq):\n",
        "    return token.sequences_to_texts([seq])\n",
        "\n",
        "response = greedy_search_withT(question, model, max_len=15, T=1.1)\n",
        "rta = from_seq_to_text(response)\n",
        "rta"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cc5_Ya5lPdV-",
        "outputId": "bb7c98ba-e145-4a28-f62a-8502c46f5673",
        "execution": {
          "iopub.status.busy": "2023-11-02T14:48:44.494586Z",
          "iopub.execute_input": "2023-11-02T14:48:44.495408Z",
          "iopub.status.idle": "2023-11-02T14:48:46.114830Z",
          "shell.execute_reply.started": "2023-11-02T14:48:44.495375Z",
          "shell.execute_reply": "2023-11-02T14:48:46.113894Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 175,
          "output_type": "execute_result",
          "data": {
            "text/plain": "['through her to this man some unfortunately choked and when he had rushed two dramatic']"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stochastic beam search + T"
      ],
      "metadata": {
        "id": "ucj0WhQ3bt5k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Por ultimo, se implemento una funcion recursiva que ejecuta el algoritmo *stochastic beam search*. Se hicieron muchas pruebas con este algoritmo, ya que era de esperar que entre este y anterior sean los algoritmos que mejor debian generar texto. Luego de varias pruebas, encontramos cierta coherencia en la generacion, sobre todo para la red LSTM. Sin embargo, existen ejemplos como los que estas debajo donde la generacion no es muy coherente."
      ],
      "metadata": {
        "id": "H2z6RjznM0ym"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def stoch_beam_search(kBeams, maxOutputLen, candidatesAndProb, model, T=1.0):\n",
        "\n",
        "  candidates = candidatesAndProb[0]\n",
        "  probabilities = candidatesAndProb[1]\n",
        "\n",
        "  if maxOutputLen < 1:\n",
        "    return candidates, probabilities\n",
        "  else:\n",
        "    possibleCand = [0]*kBeams\n",
        "    possibleProb = [0]*kBeams\n",
        "\n",
        "    #Calculamos los candidatos para cada rama, kBeams candidatos maximos\n",
        "    for curr in candidates:\n",
        "      out = pad_sequences([curr], maxlen=maxLen)\n",
        "      out = model.predict(out, verbose=0) #Probabilidades\n",
        "      temp = changeTemp(out[0], temperature=T)\n",
        "      newCandidates = np.argsort(temp)[::-1][:kBeams] #Agarramos los indices de los k-elementos con mas prob\n",
        "      newCandidates = [[num] for num in newCandidates] #Convertimos los numeros en listas de numeros por compatibilidad con pad_sequences\n",
        "      newCandProba = [temp[i] for i in newCandidates] #Agarramos su probabilidad\n",
        "\n",
        "      #Elegimos los kBeams caminos con mas probabilidad\n",
        "      for i, prob in enumerate(newCandProba): #Agarramos la probabilidad de cada candidato\n",
        "        for j in range(len(possibleProb)): #Para cada probabilidad maxima guardada\n",
        "          if possibleProb[j] < prob and newCandProba[i] not in possibleCand:\n",
        "            possibleProb[j] = prob\n",
        "            possibleCand[j] = newCandProba[i]\n",
        "\n",
        "    concatenated = np.concatenate((candidates, newCandidates), axis=1)\n",
        "    probabilities = np.multiply(probabilities, newCandProba)\n",
        "\n",
        "    return stoch_beam_search(kBeams, maxOutputLen-1, (concatenated, probabilities), model)"
      ],
      "metadata": {
        "id": "wH5B8sP2bpQw",
        "execution": {
          "iopub.status.busy": "2023-11-02T14:49:05.003920Z",
          "iopub.execute_input": "2023-11-02T14:49:05.004431Z",
          "iopub.status.idle": "2023-11-02T14:49:05.014548Z",
          "shell.execute_reply.started": "2023-11-02T14:49:05.004386Z",
          "shell.execute_reply": "2023-11-02T14:49:05.013579Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = token.texts_to_sequences([\"<sos> watson is\"])"
      ],
      "metadata": {
        "id": "PtnJvuQAbqjA",
        "execution": {
          "iopub.status.busy": "2023-11-02T14:53:50.457659Z",
          "iopub.execute_input": "2023-11-02T14:53:50.458303Z",
          "iopub.status.idle": "2023-11-02T14:53:50.462450Z",
          "shell.execute_reply.started": "2023-11-02T14:53:50.458272Z",
          "shell.execute_reply": "2023-11-02T14:53:50.461527Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "maxOutputLen = 10\n",
        "kBeams = 7\n",
        "candidates = (question*kBeams, 1*kBeams)\n",
        "\n",
        "outSeq, outProb = stoch_beam_search(kBeams, maxOutputLen, candidates, model=model, T=0.9)\n",
        "result = token.sequences_to_texts(outSeq)\n",
        "\n",
        "for i, sent in enumerate(result):\n",
        "  print(f\"prob: {outProb[i][0]:.10f} - Rta: {sent}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "whbxNptz51Lp",
        "outputId": "4a29f9e2-0b91-41d1-8378-81178d9958e9",
        "execution": {
          "iopub.status.busy": "2023-11-02T14:53:51.710819Z",
          "iopub.execute_input": "2023-11-02T14:53:51.711506Z",
          "iopub.status.idle": "2023-11-02T14:53:58.637615Z",
          "shell.execute_reply.started": "2023-11-02T14:53:51.711471Z",
          "shell.execute_reply": "2023-11-02T14:53:58.636668Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "prob: 0.0000000058 - Rta: <sos> watson is a to to holmes said been be to to the\nprob: 0.0000000000 - Rta: <sos> watson is the that that he i a the the and a\nprob: 0.0000000000 - Rta: <sos> watson is not not i i was not have in in my\nprob: 0.0000000000 - Rta: <sos> watson is it the not that is the me and for it\nprob: 0.0000000000 - Rta: <sos> watson is no said much the to no do out at me\nprob: 0.0000000000 - Rta: <sos> watson is very a a you had done see up of his\nprob: 0.0000000000 - Rta: <sos> watson is you so said she have to go back with him\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def perplexity_old(model, init_seq, length):\n",
        "  seq = init_seq\n",
        "  score = 1\n",
        "  for i in range(length):\n",
        "    out = pad_sequences(seq, maxlen=maxLen)\n",
        "    out = model.predict(out, verbose=0)\n",
        "    argmax = np.argmax(out[0])\n",
        "    seq = [np.concatenate([seq[0], [argmax]])]\n",
        "    print(seq)\n",
        "    score *= (1/out[0][argmax])**(1/length)\n",
        "  return score\n",
        "\n",
        "def perplexity(model, init_seq):\n",
        "  seq = init_seq\n",
        "  score = 1\n",
        "  for i in range(2, len(seq[0])):\n",
        "    out = pad_sequences([seq[0][:i]], maxlen=maxLen)\n",
        "    out = model.predict(out, verbose=0)\n",
        "    score *= (1/out[0][seq[0][i]])**(1/len(seq[0]))\n",
        "  return score"
      ],
      "metadata": {
        "id": "-hKxXPaH45kU",
        "execution": {
          "iopub.status.busy": "2023-11-02T14:39:04.961679Z",
          "iopub.execute_input": "2023-11-02T14:39:04.962406Z",
          "iopub.status.idle": "2023-11-02T14:39:04.970706Z",
          "shell.execute_reply.started": "2023-11-02T14:39:04.962371Z",
          "shell.execute_reply": "2023-11-02T14:39:04.969681Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "En los ejemplos de abajo podemos observar como a partir de la perplejidad podemos determinar que tanta certeza tiene la red de una oracion. Por ejemplo, es de esperar que siendo que Sherlock Holmes es un detective segun el dataset, la red de una perplejidad mas pequeña que para los casos en donde le cambiamos el oficio o afirmamos que es una mascota. Ademas, vemos que la perplejidad para el caso donde decimos que Sherlock era un hombre es mucho mas baja que en los otros casos, indicando que en las palabras existe cierto conocimiento del contexto."
      ],
      "metadata": {
        "id": "J9yT4plBQ9v2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question = token.texts_to_sequences([\"<sos> sherlock holmes was a detective\"])\n",
        "print(question)\n",
        "score = perplexity(model, question)\n",
        "score"
      ],
      "metadata": {
        "id": "l04yAxMG45kU",
        "outputId": "e0fea0cf-e6d7-463a-acef-428b3acb3c6b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.status.busy": "2023-11-02T14:35:57.550424Z",
          "iopub.execute_input": "2023-11-02T14:35:57.550804Z",
          "iopub.status.idle": "2023-11-02T14:35:57.956995Z",
          "shell.execute_reply.started": "2023-11-02T14:35:57.550774Z",
          "shell.execute_reply": "2023-11-02T14:35:57.956033Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "[[1, 123, 36, 14, 8, 1563]]\n",
          "output_type": "stream"
        },
        {
          "execution_count": 94,
          "output_type": "execute_result",
          "data": {
            "text/plain": "8.51712392232643"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question = token.texts_to_sequences([\"<sos> sherlock holmes was a doctor\"])\n",
        "print(question)\n",
        "score = perplexity(model, question)\n",
        "score"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-02T14:36:43.008673Z",
          "iopub.execute_input": "2023-11-02T14:36:43.009707Z",
          "iopub.status.idle": "2023-11-02T14:36:43.400395Z",
          "shell.execute_reply.started": "2023-11-02T14:36:43.009667Z",
          "shell.execute_reply": "2023-11-02T14:36:43.399370Z"
        },
        "trusted": true,
        "id": "3mt3rc64IQ2H",
        "outputId": "dcfd1a62-6632-42e7-bfc6-41085c614e1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "[[1, 123, 36, 14, 8, 345]]\n",
          "output_type": "stream"
        },
        {
          "execution_count": 97,
          "output_type": "execute_result",
          "data": {
            "text/plain": "10.886164281166694"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question = token.texts_to_sequences([\"<sos> sherlock holmes was a man\"])\n",
        "print(question)\n",
        "score = perplexity(model, question)\n",
        "score"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-02T14:36:45.190518Z",
          "iopub.execute_input": "2023-11-02T14:36:45.191233Z",
          "iopub.status.idle": "2023-11-02T14:36:45.613600Z",
          "shell.execute_reply.started": "2023-11-02T14:36:45.191201Z",
          "shell.execute_reply": "2023-11-02T14:36:45.612535Z"
        },
        "trusted": true,
        "id": "A4brDsxeIQ2H",
        "outputId": "b1e78a70-30a1-42ce-c101-ad6e938e6dba"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "[[1, 123, 36, 14, 8, 59]]\n",
          "output_type": "stream"
        },
        {
          "execution_count": 98,
          "output_type": "execute_result",
          "data": {
            "text/plain": "4.620224526174586"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question = token.texts_to_sequences([\"<sos> sherlock holmes was a pet\"])\n",
        "print(question)\n",
        "score = perplexity(model, question)\n",
        "score"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-02T14:39:10.470021Z",
          "iopub.execute_input": "2023-11-02T14:39:10.470395Z",
          "iopub.status.idle": "2023-11-02T14:39:10.857504Z",
          "shell.execute_reply.started": "2023-11-02T14:39:10.470365Z",
          "shell.execute_reply": "2023-11-02T14:39:10.856584Z"
        },
        "trusted": true,
        "id": "uPIut6LCIQ2H",
        "outputId": "3111cf75-31dc-4428-8718-da6102ccf0cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "[[1, 123, 36, 14, 8, 4374]]\n",
          "output_type": "stream"
        },
        {
          "execution_count": 106,
          "output_type": "execute_result",
          "data": {
            "text/plain": "12.217431397765255"
          },
          "metadata": {}
        }
      ]
    }
  ]
}